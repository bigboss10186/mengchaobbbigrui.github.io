# 计算机视觉与深度学习

## 全连接神经网络

首先下面的流程图是整个全连接神经网络所要学习的内容。

![](https://ftp.bmp.ovh/imgs/2021/06/19030ea73e77daaf.jpg)

### 图像表示

像素表示与之前所说的相同，将不同像素点的rgb值转化为一个列向量，直接利用原始像素作为特征，例如cifar10中每个图像可表示为特征。

### 分类模型

多层感知器：

下面是多层全连接网络的表达式：

![](https://ftp.bmp.ovh/imgs/2021/06/32ee6ee058130ffb.jpg)

[![RVKxzt.jpg](https://z3.ax1x.com/2021/06/21/RVKxzt.jpg)](https://imgtu.com/i/RVKxzt)

全连接神经网络之所以特征提取能力更强就是如图所示，其中W1可以按照个人的需求自己设定，调整W1行数等于增加模板个数，W2需要匹配多个模板的结果来实现最终打分。这样分类器就可以学到更多我们想让其学到的东西。

线性分类器可以分类线性关系的数据，而神经网络可以分类处于非线性关系的数据。

全连接神经网络的命名：一般的神经网络包括输入层，隐藏层，输出层，然后神经网络的层数不包括输入层，剩下的层数就是这个神经网络的层数。如果说N隐层神经网络就和上面不一样，这里指隐层的数目为N。例如：

N层全连接神经网络：除输入层之外其他层的数量为N的网络。

N个隐层的全连接神经网络：网络隐层的数量为N的网络。

### 激活函数

首先为什么需要激活函数，就如下图所示，假如没有激活函数的话，全连接神经网络就会变成一个线性分类器。

[![RVl3x1.jpg](https://z3.ax1x.com/2021/06/21/RVl3x1.jpg)](https://imgtu.com/i/RVl3x1)

接着是常见的激活函数

[![RVlvW9.jpg](https://z3.ax1x.com/2021/06/21/RVlvW9.jpg)](https://imgtu.com/i/RVlvW9)

其中Sigmoid的激活函数使得输出的值保持在0到1之间。ReLU激活函数是最常用的激活函数，其作用就是将小于0的值归0。tanh激活函数的作用是使得输出的值保持在-1到+1之间，保持其对称性，也会保留原本的正负，最后Leaky ReLU激活函数的正半轴与ReLU相同，负半轴将其缩小为原来的十分之一。

### 网络结构设计

对于网络结构设计，应该考虑两个问题。

1.用不用隐层，用一个还是用几个隐层？（深度设计）

2.每隐层设置多少个神经元比较合适？（宽度设计）

依据分类任务的难易程度来调整神经网络模型的复杂程度。分类任务越难，我们设计的神经网络结构就应该越深，越宽。但是，需要注意的是对训练集分类精度最高的全连接神经网络模型，在真实场景下识别性能未必是最好的（过拟合）。

小结：

1.全连接神经网络的组成：一个输入层，一个输出层以及多个隐层；

2.输入层与输出层的神经元个数由任务决定，而隐层数量以及每个隐层的神经元个数需要人为指定；

3.激活函数是全连接神经网络中的一个重要部分，缺少了激活函数，全连接神经网路将退化为线性分类器。

输入值用预处理来实现零均值等操作，激活函数作用于全连接神经网络，作用于神经元，线性变换以后的输出上面的。

 ### 损失函数

SOFTMAX与交叉熵：首先介绍SOFTMAX，其中对于隐层输出的值取e的次方是和logistic回归有关，是推到出来的表达式。接着是交叉熵，我们也可以推导出交叉熵等于熵与相对熵的和。

[![RVOfr8.jpg](https://z3.ax1x.com/2021/06/22/RVOfr8.jpg)](https://imgtu.com/i/RVOfr8)

[![RZcfcn.jpg](https://z3.ax1x.com/2021/06/22/RZcfcn.jpg)](https://imgtu.com/i/RZcfcn)

对比多类支持向量机损失：

接下来，将交叉熵损失与支持向量机损失相对比可以得到：

[![RZ5fm9.jpg](https://z3.ax1x.com/2021/06/22/RZ5fm9.jpg)](https://imgtu.com/i/RZ5fm9)

[![RZ5blD.jpg](https://z3.ax1x.com/2021/06/22/RZ5blD.jpg)](https://imgtu.com/i/RZ5blD)

从上面可以看到，在同样可以预测正确的情况下，不同的损失函数所计算出来的结果是不一样的，交叉熵损失对比支持向量机损失能看出更多的数据。

### 优化算法

计算图与反向传播

计算图是一种有向图，它用来表达输入输出以及中间变量之间的计算关系，每一个节点对应着一个数学运算。

计算图总结：

1.任意复杂的函数，都可以用计算图的形式总结。

2.在整个计算图中，每个门单元都会得到一些输入，然后，进行下面两个计算：

a）这个门的输入值。

b）其输出值关于输入值的局部梯度

3.利用链式法则，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输出值的梯度。

计算图的颗粒度：可以将表达式直接求导，这样颗粒度比较大，计算效率快。也可以按照公式逐步求导，这样计算下来颗粒度比较小，但是计算效率不高。

常见门单元如下：

[![RJvdyt.jpg](https://z3.ax1x.com/2021/06/27/RJvdyt.jpg)](https://imgtu.com/i/RJvdyt)

再谈激活函数

动量法与自适应梯度



 



