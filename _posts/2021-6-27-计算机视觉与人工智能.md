---
layout:     post
title:      计算机与人工智能
subtitle:   学习日记（2）
date:       2021-06-27
author:     BY Bigboss
header-img: img/pexels-diana-smykova-8369439.jpg
catalog: 	 true
tags:
    - Learning diary
---
# 计算机视觉与深度学习

## 全连接神经网络

首先下面的流程图是整个全连接神经网络所要学习的内容。

![](https://ftp.bmp.ovh/imgs/2021/06/19030ea73e77daaf.jpg)

### 图像表示

像素表示与之前所说的相同，将不同像素点的rgb值转化为一个列向量，直接利用原始像素作为特征，例如cifar10中每个图像可表示为特征。

### 分类模型

多层感知器：

下面是多层全连接网络的表达式：

![](https://ftp.bmp.ovh/imgs/2021/06/32ee6ee058130ffb.jpg)

[![RVKxzt.jpg](https://z3.ax1x.com/2021/06/21/RVKxzt.jpg)](https://imgtu.com/i/RVKxzt)

全连接神经网络之所以特征提取能力更强就是如图所示，其中W1可以按照个人的需求自己设定，调整W1行数等于增加模板个数，W2需要匹配多个模板的结果来实现最终打分。这样分类器就可以学到更多我们想让其学到的东西。

线性分类器可以分类线性关系的数据，而神经网络可以分类处于非线性关系的数据。

全连接神经网络的命名：一般的神经网络包括输入层，隐藏层，输出层，然后神经网络的层数不包括输入层，剩下的层数就是这个神经网络的层数。如果说N隐层神经网络就和上面不一样，这里指隐层的数目为N。例如：

N层全连接神经网络：除输入层之外其他层的数量为N的网络。

N个隐层的全连接神经网络：网络隐层的数量为N的网络。

### 激活函数

首先为什么需要激活函数，就如下图所示，假如没有激活函数的话，全连接神经网络就会变成一个线性分类器。

[![RVl3x1.jpg](https://z3.ax1x.com/2021/06/21/RVl3x1.jpg)](https://imgtu.com/i/RVl3x1)

接着是常见的激活函数

[![RVlvW9.jpg](https://z3.ax1x.com/2021/06/21/RVlvW9.jpg)](https://imgtu.com/i/RVlvW9)

其中Sigmoid的激活函数使得输出的值保持在0到1之间。ReLU激活函数是最常用的激活函数，其作用就是将小于0的值归0。tanh激活函数的作用是使得输出的值保持在-1到+1之间，保持其对称性，也会保留原本的正负，最后Leaky ReLU激活函数的正半轴与ReLU相同，负半轴将其缩小为原来的十分之一。

### 网络结构设计

对于网络结构设计，应该考虑两个问题。

1.用不用隐层，用一个还是用几个隐层？（深度设计）

2.每隐层设置多少个神经元比较合适？（宽度设计）

依据分类任务的难易程度来调整神经网络模型的复杂程度。分类任务越难，我们设计的神经网络结构就应该越深，越宽。但是，需要注意的是对训练集分类精度最高的全连接神经网络模型，在真实场景下识别性能未必是最好的（过拟合）。

小结：

1.全连接神经网络的组成：一个输入层，一个输出层以及多个隐层；

2.输入层与输出层的神经元个数由任务决定，而隐层数量以及每个隐层的神经元个数需要人为指定；

3.激活函数是全连接神经网络中的一个重要部分，缺少了激活函数，全连接神经网路将退化为线性分类器。

输入值用预处理来实现零均值等操作，激活函数作用于全连接神经网络，作用于神经元，线性变换以后的输出上面的。

 ### 损失函数

SOFTMAX与交叉熵：首先介绍SOFTMAX，其中对于隐层输出的值取e的次方是和logistic回归有关，是推到出来的表达式。接着是交叉熵，我们也可以推导出交叉熵等于熵与相对熵的和。

[![RVOfr8.jpg](https://z3.ax1x.com/2021/06/22/RVOfr8.jpg)](https://imgtu.com/i/RVOfr8)

[![RZcfcn.jpg](https://z3.ax1x.com/2021/06/22/RZcfcn.jpg)](https://imgtu.com/i/RZcfcn)

对比多类支持向量机损失：

接下来，将交叉熵损失与支持向量机损失相对比可以得到：

[![RZ5fm9.jpg](https://z3.ax1x.com/2021/06/22/RZ5fm9.jpg)](https://imgtu.com/i/RZ5fm9)

[![RZ5blD.jpg](https://z3.ax1x.com/2021/06/22/RZ5blD.jpg)](https://imgtu.com/i/RZ5blD)

从上面可以看到，在同样可以预测正确的情况下，不同的损失函数所计算出来的结果是不一样的，交叉熵损失对比支持向量机损失能看出更多的数据。

### 优化算法

计算图与反向传播

计算图是一种有向图，它用来表达输入输出以及中间变量之间的计算关系，每一个节点对应着一个数学运算。

计算图总结：

1.任意复杂的函数，都可以用计算图的形式总结。

2.在整个计算图中，每个门单元都会得到一些输入，然后，进行下面两个计算：

a）这个门的输入值。

b）其输出值关于输入值的局部梯度

3.利用链式法则，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输出值的梯度。

计算图的颗粒度：可以将表达式直接求导，这样颗粒度比较大，计算效率快。也可以按照公式逐步求导，这样计算下来颗粒度比较小，但是计算效率不高。

常见门单元如下：

[![RJvdyt.jpg](https://z3.ax1x.com/2021/06/27/RJvdyt.jpg)](https://imgtu.com/i/RJvdyt)

再谈激活函数

梯度消失：梯度消失是神经网络训练中非常致命的一个问题，其本质是由于链式法则的乘法特性导致的。

梯度爆炸：断崖处梯度乘以学习率后会是一个非常大的值，从而“飞”出了合理区域，最终导致算法不收敛。解决办法包括把沿梯度方向前进的步长限制在某个值内就可以避免“飞”出了，这个方法也称为梯度裁剪。

sigmoid函数使用频率不高的原因：其梯度情况不好，最高的梯度是0.25，如果神经网络的层数比较多的话就会出现梯度消失现象。双曲正切函数tanh（x）与sigmoid类似，局部梯度特性不利于网络梯度流的反向传播。

Relu函数：当输入大于0时，局部梯度永远不会为0，比较有利于梯度流的传递。

Leakly Relu函数：基本没有死区，也就是梯度永远不会为0。之所以说基本，是因为函数在0处没有导数。

激活函数结论：尽量选择Relu函数或者Leakly ReLU函数，相对于Sigmoid或者是tanh，ReLU函数或者Leakly ReLU函数会让梯度流更加顺畅，训练过程收敛更快。

动量法与自适应梯度：

梯度下降法存在的问题1）损失函数特性：一个方向上变化迅速而在另一个方向上变化缓慢。

​                  2）优化目标：从起点处走到底端笑脸处。

​                  3）梯度下降算法存在的问题：山壁间震荡，往谷低方向的行进较慢。

动量法介绍：

[![RscRN4.jpg](https://z3.ax1x.com/2021/07/01/RscRN4.jpg)](https://imgtu.com/i/RscRN4)

自适应梯度法：

自适应梯度法通过减小震荡方向步长，增大平坦方向步长来减小震荡，加速通往谷低方向。自适应梯度法通过减小震荡方向步长，增大平坦方向步长来减小震荡，加速通往谷低方向。关于震荡方向和谷低方向的划分，梯度幅度的平方较大的方向是震荡方向，梯度幅度较小的方向是平坦方向。

![](https://ftp.bmp.ovh/imgs/2021/07/9528324b5d9522a8.jpg)



Adam是上面两种算法的合并。将两种算法的优点合并在一起。

![](https://ftp.bmp.ovh/imgs/2021/07/ed36cad2779e7ea6.jpg)



权值初始化小结

1.好的初始化方法可以防止前向传播过程中的信息消失，也可以解决反向传播过程中的梯度消失。

2.激活函数选择双曲正切或Sigmoid时，建议使用Xaizer初始化方法。

3.激活函数选择ReLU或Leakly ReLU时，推荐使用He初始化方法。

批归一化：

直接对神经元的输出进行批归一化。调整权值分布使得输出与输入具有相同的分布。如果每一层的每个神经元进行归一化，就能解决前向传递过程中的信号消失问题。

![](https://ftp.bmp.ovh/imgs/2021/07/aeebfbbb15d9ccc1.jpg)

上面图中第一问答案：根据对分类的贡献自行决定数据分布的均值和方差。

第二问答案：来自与训练中，累加训练时每个批次的均值和方差，最后进行平均，用平均后的结果作为预测时的均值和方差。

过拟合与欠拟合：

过拟合--是指学习时选择的模型所包含的参数过多，以至于出现这一模型对已知数据预测的很好，但对未知数据预测得很差的现象。这种情况下模型可能只是记住了训练集数据，而不是学习到了数据特性。

欠拟合--模型描述能力太弱，以至于不能很好地学习到数据中的规律。产生欠拟合的原因通常是模型过于简单。

机器学习的根本问题是优化和泛化的问题，优化--是指调节模型以在训练数据上得到最佳性能。泛化--是指训练好的模型在前所未见的数据上的性能好坏。

训练初期：优化和泛化是相关的，训练集上的误差越小，模型的泛化能力逐渐增强。

训练后期：模型在验证集上的错误率不再降低，转而开始变高。模型出现过拟合，开始学习仅和训练数据有关的模式。

关于过拟合的解决方法：最优方案：获取更多的训练数据。最优方案：调节模型允许存储的信息量或者对模型允许存储的信息加以约束，该类方法也称为正则化。

实际方案：1）调节模型大小

​         2）约束模型权重，即权重正则化（常用的有L1，L2正则化）

​         3）随机失活（Dropout）

随机失活为什么能够防止过拟合？

答：有三种解释。1）随机失活使得每次更新梯度时参与计算的网络参数减少了，降低了模型容量，所以能防止过拟合。2）随机失活激励权重分散，从这个角度来看随机失活也能起到正则化的作用，从而防止过拟合。3）Dropout可以看作模型集成。

神经网络中的超参数：

网络结构：隐层神经元个数，网络层数，非线性单元选择等

优化相关：学习率，Dropout比率，正则项强度等

学习率作为一种超参数，学习率过大，训练过程无法收敛。学习率偏大，在最小值附近震荡，达不到最优。学习率太小，收敛时间较长。学习率适中，收敛快，结果好。

超参数优化方法：

1）网络搜索法：1.每个超参数分别取几几个值，组合这些超参数值，形成多组超参数。2.在验证集上评估每组超参数的模型性能。3.选择性能最优的模型所采用的那组值作为最终的超参数的值。

2）随机搜索法：1.参数空间内随机取点，每个点对应一组超参数。2.在验证集上评估每组超参数的模型性能。3.选择性能最优的模型所采用的那组值作为最终的超参数的值。

随机搜索法是实验效果更好。

超参数搜索策略：1）粗搜索：利用随机法在较大范围里采样超参数，训练一个周期，依据验证集正确率缩小超参数范围。2）精搜索：利用随机法在前述缩小的范围内采样超参数，运行模型五到十个周期，选择验证集上精度最高的那组超参数。

建议：对于学习率，正则项强度这类超参数，在对数空间上进行随机采样更合适。


 



