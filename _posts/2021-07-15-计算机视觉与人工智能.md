---
layout:     post
title:      计算机视觉与人工智能
subtitle:   学习日记（5）
date:       2021-07-15
author:     BY Bigboss
header-img: img/gratisography-green-sneakers-03-free-stock-photo.jpg
catalog: 	 true
tags:
    - Learning diary


---

# 经典网络解析

## AlexNet

AlexNet--2012年ImageNet大规模视觉识别挑战赛冠军，精度提升超过10个百分点。关于ImageNet大规模视觉识别挑战赛有如下介绍：

[![W91L2d.jpg](https://z3.ax1x.com/2021/07/11/W91L2d.jpg)](https://imgtu.com/i/W91L2d)

[![W93Jqx.jpg](https://z3.ax1x.com/2021/07/11/W93Jqx.jpg)](https://imgtu.com/i/W93Jqx)

AlexNet主要贡献：

1.提出了一种卷积层加全连接层的卷积神经网络结构。

2.首次使用ReLU函数做为神经网络的激活函数。

3.首次提出Dropout正则化来控制过拟合。

4.使用加入动量的小批量梯度下降算法加速了训练过程的收敛。

5.使用数据增强策略极大地抑制了训练过程的过拟合。

6.利用了GPU的并行计算能力，加速了网络的训练与推断。

[![W98amq.jpg](https://z3.ax1x.com/2021/07/11/W98amq.jpg)](https://imgtu.com/i/W98amq)

接下来是我整理的AlexNet结构的卷积核尺寸及相关问题：

![](https://ftp.bmp.ovh/imgs/2021/07/0babcf3c612e9e24.jpg)

我们可以通过之前讲的计算方法得到卷积层输出的特征响应图组的大小，MAX POOL3的输出的特征响应图组的大小为6\*6\*256，将其整理为9216\*1的向量作为全连接层的输入。

重要说明：

1.用于提取图像特征的卷积层以及用于分类的全连接层是同时学习的。

2.卷积层与全连接层在学习过程中会相互影响，相互促进。

重要技巧：

1.Dropout策略防止过拟合。

2.使用加入动量的随机梯度下降算法，加速收敛。

3.验证集损失不下降时，手动降低10倍的学习率。

4.采用样本增强策略增加训练样本数量，防止过拟合。

5.集成多个模型，进一步提高精度。

一些说明：

1.AlexNet分布于两个GPU，每个GPU各有一半神经元。因为在ALexNet提出的时候显卡的显存过小，导致必须要将其分布在两个GPU进行同时计算，我们现在在使用的时候可以将其直接放在一个GPU中进行计算。

2.CONV1，CONV2，CONV4，CONV5仅适用在同一GPU的前层网络输出的特征图作为输入。

3.CONV3，FC6，FC7，FC8拼接前一层所有的特征图，实现了神经网络的跨GPU计算。

关于具体卷积层在做什么，我们可以通过下面的例子进行理解：

![](https://ftp.bmp.ovh/imgs/2021/07/90cab5cbda31a760.jpg)



##  ZFNet

![45.jpg](https://i.loli.net/2021/07/11/mnvTpwLebxr2zl4.jpg)



## VGG

VGG网络结构:

![46.jpg](https://i.loli.net/2021/07/11/chZdMoFbmEGzP8y.jpg)

VGG网络贡献：

1.使用尺寸更小的3\*3卷积核串联来获得更大的感受野。

2.放弃使用11\*11和5\*5这样的大尺寸卷积核。

3.深度更深，非线性更强，网络的参数也更少。

4.去掉了AlexNet中的局部响应归一化层。

VGG16与VGG19对比：19层VGG更深，精度略微，但需要内存更多。VGG16更常用。

VGG16：

1.13个卷积层与3个全连接。

2.分为5段conv1，...，conv5，每一段中卷积层的卷积核个数均相同。

3.卷积层均采用3\*3的卷积核及ReLU激活函数。

4.池化层均采用最大池化，其窗口大小为2\*2,步长为2。

5.经过一次池化操作，气候卷积层的卷积核个数就增加一倍，直至到达512。

6.经过一次池化操作，其后卷积层的卷积核个数就增加一倍，直至到达512。

7.全连接层中也使用了Dropout策略。

思考：

小卷积核有哪些优势？

多个小尺寸卷积核串联可以得到大尺寸卷积核相同的感受野。使用小卷积核串联构建的网络深度更深，非线性更强，参数也更少。

为什么VGG网络前四段里，每经过一次池化操作，卷积核个数就增加一倍？

1.池化操作可以减少特征图尺寸，降低显存占用。

2.增加卷积核个数有助于学习更多的结构特征，但会增加网络参数数量以及内存消耗。

3.一减一增的设计平衡了识别精度与存储，计算开销。

最终提升了网络性能。

为什么卷积核个数增加到512后就不再增加了？

1.第一个全连接层含102M参数，占总参数个数的74%。

2.这一层的参数个数是特征图的尺寸与个数的乘积。

3.参数过多容易过拟合，且不易被训练。

## GoogleNet

![47.jpg](https://i.loli.net/2021/07/11/fiW6OksZqCAxvYV.jpg)

串联结构存在的问题：

后面的卷积层只能处理前层输出的特征图，前层丢失重要信息，后层无法找回。

解决方法：每一层尽量多的保留输入信号中的信息。

![48.jpg](https://i.loli.net/2021/07/11/1roNqHidzOjEeQf.jpg)



## ResNet

持续向一个“基础”的卷积神经网络上面叠加更深的层数会发生什么？

会发现在训练集和测试集上面56层的错误率都会大于20层的错误率。对于这种现象，我们的猜测是加深网络层数引起过拟合，导致错误率上升。实际原因则是训练过程中网络的正反信息流动不顺畅，网络没有被充分训练。

ResNet网络的贡献：

1.提出了一种残差模块，通过堆叠残差模块可以构建任意深度的神经网络，而不会出现“退化”现象。

2.提出了批归一化来对抗梯度消失，该方法降低了网络训练过程对于权重初始化的依赖。

3.提出了一种针对ReLU激活函数的初始化方法。

关于残差网络，下面是具体的介绍：

[![WnQpeU.jpg](https://z3.ax1x.com/2021/07/15/WnQpeU.jpg)](https://imgtu.com/i/WnQpeU)

关于残差结构：

1.残差结构能够避免普通的卷积层堆叠存在信息丢失问题，保证前向信息流的顺畅。

2.残差结构能够应对梯度反传过程中的梯度消失问题，保证反向梯度流的通顺。

关键点：

1.提出了批归一化策略，降低了网络训练过程对于权重初始化的依赖。

2.提出了针对ReLU非线性单元的初始化方法。

[![WuSeBV.md.jpg](https://z3.ax1x.com/2021/07/15/WuSeBV.md.jpg)](https://imgtu.com/i/WuSeBV)



小结：

1.介绍了5种经典的卷积神经网络AlexNet、ZFNet、VGG、GoogLeNet和ResNet。

2.残差网络和Inception V4是公认的推广性能最好的两个分类模型。

3.特殊应用环境下的模型：面向有限存储资源的SqueezeNet以及面向有限计算资源的MobileNet和ShuffleNet。
