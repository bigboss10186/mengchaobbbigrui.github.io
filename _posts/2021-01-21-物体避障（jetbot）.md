---
layout:     post
title:      物体避障及指定物体跟随（jetbot）
subtitle:   jetbot
date:       2021-1-21
author:     BY Bigboss
header-img: img/child-5943325_1920.jpg
catalog: 	 true
tags:
    - experiment
---
# 物体避障及指定物体跟随（jetbot）

​	首先展示一下自己做的jetbot，这是nvidia公司的一个开源项目，具体细节可以参考https://github.com/NVIDIA-AI-IOT/jetbot

![](https://ftp.bmp.ovh/imgs/2021/02/f42d4fd1c7eac868.jpg)

​	电路图如下：

![](https://ftp.bmp.ovh/imgs/2021/02/6956bb6437789730.png)

## 概述

​	使用到的基本的工具集包括Pytorch & torchvision 、Opencv3 & Numpy 以及cuda 。Pytorch是Facebook开源的一个在深度学习领域有强大功能的工具包，用它来搭建和训练神经网络；Torchvision是Pytorch的伴随工具包，它包含了许多数据集加载方法和模型加载方法以及其他许多辅助功能。Opencv3 主要用于对图像的简单处理，例如色彩模式的调节，图像大小格式的调节，使得获取的图片能够符合模型的输入标准；Cuda 是调用GPU的工具，通过它可以让GPU计算训练过程，加快计算速度，减少训练时间。

​	芯片方面选择的是 NVIDIA 公司的 jetson nano 深度学习开发板，用它来运行我们训练好的模型和控制电机、摄像头等硬件设备。使得小车能够实现基本的前后左右的行走，拍摄图像的功能，以及自主运行过程中的自动避障、智能跟随人类的功能。通过nano可以控制电机的驱动，从而控制左右两个电机的运行状态，来达到基本的动作实现的目标。

​	接下来介绍一下避障方面

## 数据采集

​	为了让小车实现避障，我们需要进行相应数据的采集，最重要的是对照片进行采集，我们将小车放到实验室环境中，拍摄小车处于安全和危险两种状态下的照片，其中安全的意思是小车可以前行，危险的照片是在摄像头中出现这种情形时是不能继续前行的状态，分别放入两个文件夹中，命名为‘’free‘’和‘’blocked‘’，进行压缩后进行模型训练。

## 模型训练

​	解压后的照片会分为训练集和测试集，其中训练集用于训练模型，测试集的数据用于分析模型的准确度。torchvision提供了一系列我们可以使用的经过训练的模型。在一个被称为迁移学习的过程中，我们可以重新使用一个预先训练过的模型(在数百万张图像上训练过)，来完成一个可能没有那么多可用数据的新任务。在训练前模型的原始训练中学习到的重要特征在新的任务中可以重复使用。

​	我们选用的是alexnet模型，alexnet模型最初是为有1000个类标签的数据集训练的，但是我们的数据集只有两个类标签，我们将用一个新的、未经训练的、只有两个输出的层替换最终的层。

​	最终，我们在GPU上面进行网络的训练，下面是我进行训练的时候得到的数据，可以看到只是几次训练后模型的精准度就很高，原因可能是我进行训练的数据太少了。

![](https://ftp.bmp.ovh/imgs/2021/02/b4262bf1ab2d58c3.png)

![](https://ftp.bmp.ovh/imgs/2021/02/21dd3836be60dc1a.png)

## 实现避障

​	首先从之前得到的文件里面加载训练的权重，在将CPU中的权重加载到GPU的时候要创建预处理函数，这样参数才能匹配。接着初始化摄像头，当判断目前位置处于‘’blocked‘’时，小车左转，躲避障碍。

## 跟随

​	为了实现跟随功能，我们引用了已经训练好的COCO数据集，这能帮助我们检测90种不同的物体，包括人，杯子等等，该模型来自TensorFlow对象检测API，它还提供了用于训练对象检测器的工具，可以实现一些自定义任务，模型得到训练后，我们就在Jetson Nano上使用NVIDIA TensorRT对它进行优化。

​	初始化相机后，一旦在镜头前出现能被检测到的物体，就会被储存下来，摄像头拍摄画面会对物体进行处理，得到物体的标签，置信度和边框，要控制机器人跟随物体，有几个点要做到

1. 检测与指定类匹配的对象

2. 选择最接近相机视野中心的物体，这就是“目标”物体

3. 引导机器人朝向目标物体，否则会出现飘逸等现象

4. 如果出现避障中的情况，机器人自动左拐

​  最后，我采用了跟随人的模式，整个的实验过程为：

​  首先，运行 nano ，开启摄像头，当摄像头正常工作并采集到图像时，会实时将采集到的每一帧画面传回到开发板，交给主程序进行分析。主程序获得图像信息后，首先会进行初步的处理，使它能够作为模型的输入数据，先由障碍识别模型进行分析，判断前方环境是否安全，否则左转以避开，然后再将图像数据交给目标检测的模型，由它来分析视线中的物体有没有人类，如果有的话，返回视线中最近的人所处的位置（也就是视野中心）在视线中的坐标，然后根据这一组坐标，调节小车的方向，使小车正对着最近的人前进。若前方既没有障碍物也没有跟随目标，则保持直行前进，直到遇到障碍或者目标。至此，便是我们实现智能跟随功能的基本流程。

## 总结

​	我们通过这样的算法实现的避障一定程度上可以自己认为定义危险，更加智能的实现了避障的功能，比如在‘’blocked‘’文件中加入桌边的图片时，小车会实现防坠落的功能，这样的功能是普通机器用过红外线等外设所实现不了的。跟随物体方面更加智能，能够指定特定的物体进行跟随。





